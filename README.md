# ğŸ“š Simple Local RAG System (Node.js + Ollama + SQLite)

A minimal Retrieval-Augmented Generation (RAG) system built locally
using:

-   Node.js (Express)
-   Ollama (LLM + Embedding model)
-   SQLite (file-based vector storage)
-   Simple HTML frontend with loading state

This project demonstrates the complete AI application lifecycle:

Ingest â†’ Embed â†’ Store â†’ Retrieve â†’ Generate â†’ Respond

------------------------------------------------------------------------

# ğŸ— Architecture Overview

User â†’ Frontend â†’ Node API\
â†“\
SQLite (rag.db file)\
â†“\
Ollama (Embeddings + LLM)

No Docker. No external database server.

------------------------------------------------------------------------

# ğŸ“¦ Prerequisites

## 1ï¸âƒ£ Install Node.js

https://nodejs.org/

Verify: node -v\
npm -v

## 2ï¸âƒ£ Install Ollama

https://ollama.com

Verify: ollama --version

## 3ï¸âƒ£ Pull Required Models

ollama pull phi3:mini\
ollama pull nomic-embed-text

------------------------------------------------------------------------

# ğŸš€ Setup Instructions

## 1ï¸âƒ£ Create Project

mkdir local-rag\
cd local-rag\
npm init -y

## 2ï¸âƒ£ Install Dependencies

npm install express axios body-parser better-sqlite3

## 3ï¸âƒ£ Add Files

Create:

-   server.js\
-   index.html

------------------------------------------------------------------------

# ğŸ§  Backend (server.js)

Features:

-   Stores embeddings in SQLite (`rag.db`)
-   Computes cosine similarity manually
-   Retrieves top 3 matching documents
-   Sends context to LLM
-   Returns final generated answer

Start server:

node server.js

Server runs at:

http://localhost:3000

------------------------------------------------------------------------

# ğŸ“¥ Ingest Documents

POST /ingest

Example JSON:

{ "id": "doc1", "text": "The sun is a star at the center of the solar
system." }

------------------------------------------------------------------------

# â“ Ask Questions

POST /ask

Example JSON:

{ "question": "What is the sun?" }

Flow:

1.  Question â†’ Embedded using Ollama\
2.  All stored embeddings loaded from SQLite\
3.  Cosine similarity computed in Node\
4.  Top matches selected\
5.  Context + Question sent to LLM\
6.  Final answer returned

------------------------------------------------------------------------

# ğŸ’¾ Database

A file named:

rag.db

Will be created automatically in your project folder.

This file stores:

-   Document ID\
-   Document text\
-   Embedding vector (JSON format)

------------------------------------------------------------------------

# ğŸ§ª How To Run

1ï¸âƒ£ Start Ollama

ollama serve

2ï¸âƒ£ Start Node

node server.js

3ï¸âƒ£ Open browser

http://localhost:3000

------------------------------------------------------------------------

# ğŸ§  Models Used

## LLM: phi3:mini

-   Lightweight
-   Fast
-   Good for local machines

## Embedding Model: nomic-embed-text

-   Converts text â†’ vector embeddings
-   Used for similarity search

------------------------------------------------------------------------

# ğŸ“Š What This Project Demonstrates

âœ” Manual embedding generation\
âœ” Storing embeddings in SQLite\
âœ” Implementing cosine similarity\
âœ” Building a vector search system from scratch\
âœ” Understanding how RAG works internally\
âœ” Local AI application deployment

------------------------------------------------------------------------

# ğŸ¯ When To Use This Architecture

Best for:

-   Learning RAG
-   Small datasets (\<10k documents)
-   Local development
-   Windows servers without Docker

Not ideal for:

-   Millions of vectors
-   High-performance ANN indexing
-   Distributed systems

------------------------------------------------------------------------

# ğŸ”¥ What You Built

You now understand:

-   What embeddings are
-   How vector search works
-   What a vector database really does
-   How RAG systems are architected
-   How AI apps are built end-to-end

This is real AI infrastructure knowledge.
